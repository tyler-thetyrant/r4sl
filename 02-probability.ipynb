{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability Review\n",
    "\n",
    "We give a very brief review of some necessary probability concepts. As the treatment is less than complete, a list of references is given at the end of the chapter. For example, we ignore the usual recap of basic set theory and omit proofs and examples.\n",
    "\n",
    "\n",
    "## Probability Models\n",
    "\n",
    "When discussing probability models, we speak of random **experiments** that produce one of a number of possible **outcomes**.\n",
    "\n",
    "A **probability model** that describes the uncertainty of an experiment consists of two elements:\n",
    "\n",
    "- The **sample space**, often denoted as $\\Omega$, which is a set that contains all possible outcomes.\n",
    "-  A **probability function** that assigns to an event $A$ a nonnegative number, $P[A]$, that represents how likely it is that event $A$ occurs as a result of the experiment.\n",
    "\n",
    "We call $P[A]$ the **probability** of event $A$. An **event** $A$ could be any subset of the sample space, not necessarily a single possible outcome. The probability law must follow a number of rules, which are the result of a set of axioms that we introduce now.\n",
    "\n",
    "\n",
    "## Probability Axioms\n",
    "\n",
    "Given a sample space $\\Omega$ for a particular experiment, the **probability function** associated with the experiment must satisfy the following axioms.\n",
    "\n",
    "1. *Nonnegativity*: $P[A] \\geq 0$ for any event $A \\subset \\Omega$.\n",
    "2. *Normalization*: $P[\\Omega] = 1$. That is, the probability of the entire space is 1.\n",
    "3. *Additivity*: For mutually exclusive events $E_1, E_2, \\ldots$\n",
    "$$\n",
    "P\\left[\\bigcup_{i = 1}^{\\infty} E_i\\right] = \\sum_{i = 1}^{\\infty} P[E_i]\n",
    "$$\n",
    "\n",
    "Using these axioms, many additional probability rules can easily be derived.\n",
    "\n",
    "\n",
    "## Probability Rules\n",
    "\n",
    "Given an event $A$, and its complement, $A^c$, that is, the outcomes in $\\Omega$ which are not in $A$, we have the **complement rule**:\n",
    "\n",
    "$$\n",
    "P[A^c] = 1 - P[A]\n",
    "$$\n",
    "\n",
    "In general, for two events $A$ and $B$, we have the **addition rule**:\n",
    "\n",
    "$$\n",
    "P[A \\cup B] = P[A] + P[B] - P[A \\cap B]\n",
    "$$\n",
    "\n",
    "If $A$ and $B$ are also *disjoint*, then we have:\n",
    "\n",
    "$$\n",
    "P[A \\cup B] = P[A] + P[B]\n",
    "$$\n",
    "\n",
    "If we have $n$ mutually exclusive events, $E_1, E_2, \\ldots E_n$, then we have:\n",
    "\n",
    "$$\n",
    "P\\left[\\textstyle\\bigcup_{i = 1}^{n} E_i\\right] = \\sum_{i = 1}^{n} P[E_i]\n",
    "$$\n",
    "\n",
    "Often, we would like to understand the probability of an event $A$, given some information about the outcome of event $B$. In that case, we have the **conditional probability rule** provided $P[B] > 0$.\n",
    "\n",
    "$$\n",
    "P[A \\mid B] = \\frac{P[A \\cap B]}{P[B]}\n",
    "$$\n",
    "\n",
    "Rearranging the conditional probability rule, we obtain the **multiplication rule**:\n",
    "\n",
    "$$\n",
    "P[A \\cap B] = P[B] \\cdot P[A \\mid B] \\cdot\n",
    "$$\n",
    "\n",
    "For a number of events $E_1, E_2, \\ldots E_n$, the multiplication rule can be expanded into the **chain rule**:\n",
    "\n",
    "$$\n",
    "P\\left[\\textstyle\\bigcap_{i = 1}^{n} E_i\\right] = P[E_1] \\cdot P[E_2 \\mid E_1] \\cdot P[E_3 \\mid E_1 \\cap E_2] \\cdots P\\left[E_n \\mid \\textstyle\\bigcap_{i = 1}^{n - 1} E_i\\right]\n",
    "$$\n",
    "\n",
    "Define a **partition** of a sample space $\\Omega$ to be a set of disjoint events $A_1, A_2, \\ldots, A_n$ whose union is the sample space $\\Omega$. That is\n",
    "\n",
    "$$\n",
    "A_i \\cap A_j = \\emptyset\n",
    "$$\n",
    "\n",
    "for all $i \\neq j$, and\n",
    "\n",
    "$$\n",
    "\\bigcup_{i = 1}^{n} A_i = \\Omega.\n",
    "$$\n",
    "\n",
    "Now, let $A_1, A_2, \\ldots, A_n$ form a partition of the sample space where $P[A_i] > 0$ for all $i$. Then for any event $B$ with $P[B] > 0$ we have **Bayes' Rule**:\n",
    "\n",
    "$$\n",
    "P[A_i | B] = \\frac{P[A_i]P[B | A_i]}{P[B]} = \\frac{P[A_i]P[B | A_i]}{\\sum_{i = 1}^{n}P[A_i]P[B | A_i]}\n",
    "$$\n",
    "\n",
    "The denominator of the latter equality is often called the **law of total probability**:\n",
    "\n",
    "$$\n",
    "P[B] = \\sum_{i = 1}^{n}P[A_i]P[B | A_i]\n",
    "$$\n",
    "\n",
    "Two events $A$ and $B$ are said to be **independent** if they satisfy\n",
    "\n",
    "$$\n",
    "P[A \\cap B] = P[A] \\cdot P[B]\n",
    "$$\n",
    "\n",
    "This becomes the new multiplication rule for independent events.\n",
    "\n",
    "A collection of events $E_1, E_2, \\ldots E_n$ is said to be independent if\n",
    "\n",
    "$$\n",
    "P\\left[\\bigcap_{i \\in S} E_i \\right] = \\prod_{i \\in S}P[E_i]\n",
    "$$\n",
    "\n",
    "for every subset $S$ of $\\{1, 2, \\ldots n\\}$.\n",
    "\n",
    "If this is the case, then the chain rule is greatly simplified to:\n",
    "\n",
    "$$\n",
    "P\\left[\\textstyle\\bigcap_{i = 1}^{n} E_i\\right] = \\prod_{i=1}^{n}P[E_i]\n",
    "$$\n",
    "\n",
    "\n",
    "## Random Variables\n",
    "\n",
    "A **random variable** is simply a *function* which maps outcomes in the sample space to real numbers.\n",
    "\n",
    "\n",
    "### Distributions\n",
    "\n",
    "We often talk about the **distribution** of a random variable, which can be thought of as:\n",
    "\n",
    "$$\n",
    "\\text{distribution} = \\text{list of possible} \\textbf{ values} + \\text{associated} \\textbf{ probabilities}\n",
    "$$\n",
    "\n",
    "This is not a strict mathematical definition, but is useful for conveying the idea.\n",
    "\n",
    "If the possible values of a random variables are *discrete*, it is called a *discrete random variable*. If the possible values of a random variables are *continuous*, it is called a *continuous random variable*.\n",
    "\n",
    "\n",
    "### Discrete Random Variables\n",
    "\n",
    "The distribution of a discrete random variable $X$ is most often specified by a list of possible values and a probability **mass** function, $p(x)$. The mass function directly gives probabilities, that is,\n",
    "\n",
    "$$\n",
    "p(x) = p_X(x) = P[X = x].\n",
    "$$\n",
    "\n",
    "Note we almost always drop the subscript from the more correct $p_X(x)$ and simply refer to $p(x)$. The relevant random variable is discerned from context\n",
    "\n",
    "The most common example of a discrete random variable is a **binomial** random variable. The mass function of a binomial random variable $X$, is given by\n",
    "\n",
    "$$\n",
    "p(x | n, p) = {n \\choose x} p^x(1 - p)^{n - x}, \\ \\ \\ x = 0, 1, \\ldots, n, \\ n \\in \\mathbb{N}, \\ 0 < p < 1.\n",
    "$$\n",
    "\n",
    "This line conveys a large amount of information.\n",
    "\n",
    "- The function $p(x | n, p)$ is the mass function. It is a function of $x$, the possible values of the random variable $X$. It is conditional on the **parameters** $n$ and $p$. Different values of these parameters specify different binomial distributions.\n",
    "- $x = 0, 1, \\ldots, n$ indicates the **sample space**, that is, the possible values of the random variable.\n",
    "- $n \\in \\mathbb{N}$ and $0 < p < 1$ specify the **parameter spaces**. These are the possible values of the parameters that give a valid binomial distribution.\n",
    "\n",
    "Often all of this information is simply encoded by writing\n",
    "\n",
    "$$\n",
    "X \\sim \\text{bin}(n, p).\n",
    "$$\n",
    "\n",
    "\n",
    "### Continuous Random Variables\n",
    "\n",
    "The distribution of a continuous random variable $X$ is most often specified by a set of possible values and a probability **density** function, $f(x)$. (A cumulative density or moment generating function would also suffice.)\n",
    "\n",
    "The probability of the event $a < X < b$ is calculated as\n",
    "\n",
    "$$\n",
    "P[a < X < b] = \\int_{a}^{b} f(x)dx.\n",
    "$$\n",
    "\n",
    "Note that densities are **not** probabilities.\n",
    "\n",
    "The most common example of a continuous random variable is a **normal** random variable. The density of a normal random variable $X$, is given by\n",
    "\n",
    "$$\n",
    "f(x | \\mu, \\sigma^2) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\cdot \\exp\\left[\\frac{-1}{2} \\left(\\frac{x - \\mu}{\\sigma}\\right)^2 \\right],  \\ \\ \\ -\\infty < x < \\infty, \\ -\\infty < \\mu < \\infty, \\ \\sigma > 0.\n",
    "$$\n",
    "\n",
    "- The function $f(x | \\mu, \\sigma^2)$ is the density function. It is a function of $x$, the possible values of the random variable $X$. It is conditional on the **paramters** $\\mu$ and $\\sigma^2$. Different values of these parameters specify different normal distributions.\n",
    "- $-\\infty < x < \\infty$ indicates the sample space. In this case, the random variable may take any value on the real line.\n",
    "- $-\\infty < \\mu < \\infty$ and $\\sigma > 0$ specify the parameter space. These are the possible values of the parameters that give a valid normal distribution.\n",
    "\n",
    "Often all of this information is simply encoded by writing\n",
    "\n",
    "$$\n",
    "X \\sim N(\\mu, \\sigma^2)\n",
    "$$\n",
    "\n",
    "### Several Random Variables\n",
    "\n",
    "Consider two random variables $X$ and $Y$. We say they are independent if\n",
    "\n",
    "$$\n",
    "f(x, y) = f(x) \\cdot f(y)\n",
    "$$\n",
    "\n",
    "for all $x$ and $y$. Here $f(x, y)$ is the **joint** density (mass) function of $X$ and $Y$. We call $f(x)$ the **marginal** density (mass) function of $X$. Then $f(y)$ the marginal density (mass) function of $Y$. The joint density (mass) function $f(x, y)$ together with the possible $(x, y)$ values specify the joint distribution of $X$ and $Y$.\n",
    "\n",
    "Similar notions exist for more than two variables.\n",
    "\n",
    "\n",
    "## Expectations\n",
    "\n",
    "For discrete random variables, we define the **expectation** of the function of a random variable $X$ as follows.\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[g(X)] \\triangleq \\sum_{x} g(x)p(x)\n",
    "$$\n",
    "\n",
    "For continuous random variables we have a similar definition.\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[g(X)] \\triangleq \\int g(x)f(x) dx\n",
    "$$\n",
    "\n",
    "For specific functions $g$, expectations are given names.\n",
    "\n",
    "The **mean** of a random variable $X$ is given by\n",
    "\n",
    "$$\n",
    "\\mu_{X} = \\text{mean}[X] \\triangleq \\mathbb{E}[X].\n",
    "$$\n",
    "\n",
    "So for a discrete random variable, we would have\n",
    "\n",
    "$$\n",
    "\\text{mean}[X] = \\sum_{x} x \\cdot p(x)\n",
    "$$\n",
    "\n",
    "For a continuous random variable we would simply replace the sum by an integral.\n",
    "\n",
    "The **variance** of a random variable $X$ is given by\n",
    "\n",
    "$$\n",
    "\\sigma^2_{X} = \\text{var}[X] \\triangleq \\mathbb{E}[(X - \\mathbb{E}[X])^2] = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2.\n",
    "$$\n",
    "\n",
    "The **standard deviation** of a random variable $X$ is given by\n",
    "\n",
    "$$\n",
    "\\sigma_{X} = \\text{sd}[X] \\triangleq \\sqrt{\\sigma^2_{X}} = \\sqrt{\\text{var}[X]}.\n",
    "$$\n",
    "\n",
    "The **covariance** of random variables $X$ and $Y$ is given by\n",
    "\n",
    "$$\n",
    "\\text{cov}[X, Y] \\triangleq \\mathbb{E}[(X - \\mathbb{E}[X])(Y - \\mathbb{E}[Y])] = \\mathbb{E}[XY] - \\mathbb{E}[X] \\cdot \\mathbb{E}[Y].\n",
    "$$\n",
    "\n",
    "\n",
    "## Likelihood\n",
    "\n",
    "Consider $n$ iid random variables $X_1, X_2, \\ldots X_n$. We can then write their **likelihood** as\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\theta \\mid x_1, x_2, \\ldots x_n) = \\prod_{i = i}^n f(x_i; \\theta)\n",
    "$$\n",
    "\n",
    "where $f(x_i; \\theta)$ is the density (or mass) function of random variable $X_i$ evaluated at $x_i$ with parameter $\\theta$.\n",
    "\n",
    "Whereas a probability is a function of a possible observed value given a particular parameter value, a likelihood is the opposite. It is a function of a possible parameter value given observed data.\n",
    "\n",
    "Maximumizing likelihood is a common techinque for fitting a model to data.\n",
    "\n",
    "## Videos\n",
    "\n",
    "The YouTube channel [mathematicalmonk](https://www.youtube.com/user/mathematicalmonk) has a great [Probability Primer playlist](https://www.youtube.com/playlist?list=PL17567A1A3F5DB5E4) containing lectures on many fundamental probability concepts. Some of the more important concepts are covered in the following videos:\n",
    "\n",
    "- [Conditional Probability](https://www.youtube.com/watch?v=5BWk5qe5EJ8&index=11&list=PL17567A1A3F5DB5E4)\n",
    "- [Independence](https://www.youtube.com/watch?v=KK9jvGl9FY0&index=12&list=PL17567A1A3F5DB5E4)\n",
    "- [More Independence](https://www.youtube.com/watch?v=RMS-WglZP-c&index=13&list=PL17567A1A3F5DB5E4)\n",
    "- [Bayes Rule](https://www.youtube.com/watch?v=cM1BqBv11U8&index=14&list=PL17567A1A3F5DB5E4)\n",
    "\n",
    "## References\n",
    "\n",
    "Any of the following are either dedicated to, or contain a good coverage of the details of the topics above.\n",
    "\n",
    "- Probability Texts\n",
    "    - [Introduction to Probability](http://athenasc.com/probbook.html) by Dimitri P. Bertsekas and John N. Tsitsiklis\n",
    "    - [A First Course in Probability](https://www.pearsonhighered.com/program/Ross-First-Course-in-Probability-A-9th-Edition/PGM110742.html) by Sheldon Ross\n",
    "- Machine Learning Texts with Probability Focus\n",
    "    - [Probability for Statistics and Machine Learning](http://www.springer.com/us/book/9781441996336) by Anirban DasGupta\n",
    "    - [Machine Learning: A Probabilistic Perspective](https://mitpress.mit.edu/books/machine-learning-0) by Kevin P. Murphy\n",
    "- Statistics Texts with Introduction to Probability\n",
    "    - [Probability and Statistical Inference](https://www.pearsonhighered.com/program/Hogg-Probability-and-Statistical-Inference-9th-Edition/PGM91556.html) by Robert V. Hogg, Elliot Tanis, and Dale Zimmerman\n",
    "    - [Introduction to Mathematical Statistics](https://www.pearsonhighered.com/program/Hogg-Introduction-to-Mathematical-Statistics-7th-Edition/PGM49624.html) by Robert V. Hogg, Joseph McKean, and Allen T. Craig\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
